
# ğŸ§  NANO-VLM: Mini Vision-Language Model from Scratch

This notebook implements a **lightweight Vision-Language Model (VLM)** trained on a fully synthetic dataset of colored shapes.
It demonstrates how CLIP-style contrastive learning works using a minimal, educational pipeline.

The goal is to understand how imageâ€“text alignment models function internally â€” without relying on large pretrained transformers.

---

# ğŸ“Œ Project Overview

This notebook:

* Generates a **synthetic dataset** of images containing colored shapes
* Automatically creates corresponding **text captions**
* Builds:

  * An **Image Encoder (CNN)**
  * A **Text Encoder (Embedding + Projection)**
* Trains them using a **CLIP-style contrastive loss**
* Enables:

  * Embedding visualization
  * Text-to-image retrieval

This is a small-scale educational replica of models like CLIP, but implemented from scratch in PyTorch.

---

# ğŸ—ï¸ Notebook Structure

## 1ï¸âƒ£ Setup & Imports

* Imports PyTorch, NumPy, math, random
* Sets device (CPU/GPU)

---

## 2ï¸âƒ£ Synthetic Dataset Generation

### Dataset Properties

* Colors: red, green, blue, yellow, purple, orange, pink, brown, gray
* Shapes: (e.g., circle, square, triangle)
* Random positions

### Image Creation

The function:

```python
draw_sample(color, shape, position)
```

Generates synthetic images of colored shapes.

---

## 3ï¸âƒ£ Custom Dataset Class

```python
class ShapesDataset()
```

Responsible for:

* Generating imageâ€“caption pairs
* Tokenizing captions
* Building vocabulary
* Encoding text numerically

Dataset is split into:

* 80% Training
* 20% Validation

DataLoader is used for batching.

---

## 4ï¸âƒ£ Model Architecture

## ğŸ–¼ï¸ Image Encoder

```python
class ImageEncoder(nn.Module)
```

* Convolutional Neural Network
* Extracts image features
* Projects features into embedding space
* Output: fixed-dimensional image embedding

---

## ğŸ“ Text Encoder

```python
class TextEncoder(nn.Module)
```

* Token embedding layer
* Mean pooling / simple aggregation
* Projection to same embedding space as image encoder
* Output: fixed-dimensional text embedding

---

## ğŸ”¥ Contrastive Loss (CLIP-Style)

```python
clip_loss(img_emb, txt_emb)
```

Implements:

* Cosine similarity
* Temperature scaling
* Cross-entropy over similarity matrix

This aligns:

* Correct imageâ€“text pairs â†’ high similarity
* Incorrect pairs â†’ low similarity

---

## 5ï¸âƒ£ Training Pipeline

* Initialize encoders
* Define optimizer
* Train over batches
* Minimize contrastive loss
* Periodically evaluate

---

## 6ï¸âƒ£ Visualization Utilities

### Show Sample Images

```python
show_image()
```

### Embedding Visualization

```python
visualize_results_with_embeddings()
```

Helps inspect:

* Alignment between text and image embeddings
* Similarity structure

---

## 7ï¸âƒ£ Text-to-Image Retrieval

```python
text_to_image_retrieval(query_text)
```

Given a text query:

* Encode text
* Compute similarity with all image embeddings
* Retrieve most similar images

This demonstrates cross-modal retrieval.

---

# ğŸ§ª What This Project Demonstrates

âœ” How vision-language models align modalities
âœ” How contrastive learning works
âœ” How embedding spaces are formed
âœ” How retrieval systems operate
âœ” How CLIP-like training functions internally

---

# ğŸ›  Requirements

* Python 3.8+
* PyTorch
* NumPy
* Matplotlib

Run inside:

* Google Colab (recommended)
* Local environment with GPU (optional but helpful)

---

# â–¶ï¸ How to Run

1. Open in Google Colab
2. Run cells sequentially
3. Train model
4. Try:

```python
text_to_image_retrieval("red square")
```

---

# ğŸ¯ Learning Goals

This notebook is ideal for:

* Understanding CLIP intuitively
* Learning multimodal embedding alignment
* Building VLMs from scratch
* Research prototyping before scaling

---

# ğŸš€ Possible Extensions

* Replace simple text encoder with Transformer
* Add positional embeddings
* Increase dataset complexity
* Add attention mechanism
* Scale to real-world datasets
* Replace CNN with Vision Transformer

---

ğŸ“š Conceptual Summary

This notebook implements:

Image Encoderâ†’ ğ‘§i

Text Encoderâ†’ ğ‘§ğ‘¡ 
	â€‹

Then optimizes: Similarity(ğ‘§ğ‘–,ğ‘§ğ‘¡) 

So that matching pairs are close in embedding space.
---
# ğŸ§© Why "NANO-VLM"?

Because this is a **minimal educational version** of large vision-language models like CLIP, built entirely from scratch and small enough to fully understand.

