
# üß† NANO-VLM: Mini Vision-Language Model from Scratch

This notebook implements a **lightweight Vision-Language Model (VLM)** trained on a fully synthetic dataset of colored shapes.
It demonstrates how CLIP-style contrastive learning works using a minimal, educational pipeline.

The goal is to understand how image‚Äìtext alignment models function internally ‚Äî without relying on large pretrained transformers.

---

# üìå Project Overview

This notebook:

* Generates a **synthetic dataset** of images containing colored shapes
* Automatically creates corresponding **text captions**
* Builds:

  * An **Image Encoder (CNN)**
  * A **Text Encoder (Embedding + Projection)**
* Trains them using a **CLIP-style contrastive loss**
* Enables:

  * Embedding visualization
  * Text-to-image retrieval

This is a small-scale educational replica of models like CLIP, but implemented from scratch in PyTorch.

---

# üèóÔ∏è Notebook Structure

## 1Ô∏è‚É£ Setup & Imports

* Imports PyTorch, NumPy, math, random
* Sets device (CPU/GPU)

---

## 2Ô∏è‚É£ Synthetic Dataset Generation

### Dataset Properties

* Colors: red, green, blue, yellow, purple, orange, pink, brown, gray
* Shapes: (e.g., circle, square, triangle)
* Random positions

### Image Creation

The function:

```python
draw_sample(color, shape, position)
```

Generates synthetic images of colored shapes.

---

## 3Ô∏è‚É£ Custom Dataset Class

```python
class ShapesDataset()
```

Responsible for:

* Generating image‚Äìcaption pairs
* Tokenizing captions
* Building vocabulary
* Encoding text numerically

Dataset is split into:

* 80% Training
* 20% Validation

DataLoader is used for batching.

---

## 4Ô∏è‚É£ Model Architecture

## üñºÔ∏è Image Encoder

```python
class ImageEncoder(nn.Module)
```

* Convolutional Neural Network
* Extracts image features
* Projects features into embedding space
* Output: fixed-dimensional image embedding

---

## üìù Text Encoder

```python
class TextEncoder(nn.Module)
```

* Token embedding layer
* Mean pooling / simple aggregation
* Projection to same embedding space as image encoder
* Output: fixed-dimensional text embedding

---

## üî• Contrastive Loss (CLIP-Style)

```python
clip_loss(img_emb, txt_emb)
```

Implements:

* Cosine similarity
* Temperature scaling
* Cross-entropy over similarity matrix

This aligns:

* Correct image‚Äìtext pairs ‚Üí high similarity
* Incorrect pairs ‚Üí low similarity

---

## 5Ô∏è‚É£ Training Pipeline

* Initialize encoders
* Define optimizer
* Train over batches
* Minimize contrastive loss
* Periodically evaluate

---

## 6Ô∏è‚É£ Visualization Utilities

### Show Sample Images

```python
show_image()
```

### Embedding Visualization

```python
visualize_results_with_embeddings()
```

Helps inspect:

* Alignment between text and image embeddings
* Similarity structure

---

## 7Ô∏è‚É£ Text-to-Image Retrieval

```python
text_to_image_retrieval(query_text)
```

Given a text query:

* Encode text
* Compute similarity with all image embeddings
* Retrieve most similar images

This demonstrates cross-modal retrieval.

---

# üß™ What This Project Demonstrates

‚úî How vision-language models align modalities
‚úî How contrastive learning works
‚úî How embedding spaces are formed
‚úî How retrieval systems operate
‚úî How CLIP-like training functions internally

---

# üõ† Requirements

* Python 3.8+
* PyTorch
* NumPy
* Matplotlib

Run inside:

* Google Colab (recommended)
* Local environment with GPU (optional but helpful)

---

# ‚ñ∂Ô∏è How to Run

1. Open in Google Colab
2. Run cells sequentially
3. Train model
4. Try:

```python
text_to_image_retrieval("red square")
```

---

# üéØ Learning Goals

This notebook is ideal for:

* Understanding CLIP intuitively
* Learning multimodal embedding alignment
* Building VLMs from scratch
* Research prototyping before scaling

---

# üöÄ Possible Extensions

* Replace simple text encoder with Transformer
* Add positional embeddings
* Increase dataset complexity
* Add attention mechanism
* Scale to real-world datasets
* Replace CNN with Vision Transformer

---

# üìö Conceptual Summary

This notebook implements:

[
\text{Image Encoder} \rightarrow \mathbf{z}_i
]
[
\text{Text Encoder} \rightarrow \mathbf{z}_t
]

Then optimizes:

[
\text{Similarity}(\mathbf{z}_i, \mathbf{z}_t)
]

So that matching pairs are close in embedding space.

---

# üß© Why "NANO-VLM"?

Because this is a **minimal educational version** of large vision-language models like CLIP, built entirely from scratch and small enough to fully understand.

